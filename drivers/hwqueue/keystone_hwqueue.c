/*
 * Keystone hardware queue driver
 *
 * Copyright (C) 2011 Texas Instruments Incorporated - http://www.ti.com
 * Contact: Prabhu Kuttiyam <pkuttiyam@ti.com>
 *	    Cyril Chemparathy <cyril@ti.com>
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License
 * version 2 as published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 */

#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/device.h>
#include <linux/io.h>
#include <linux/bitops.h>
#include <linux/slab.h>
#include <linux/spinlock.h>
#include <linux/hwqueue.h>
#include <linux/platform_device.h>
#include <linux/dma-mapping.h>
#include <linux/of.h>
#include <linux/of_device.h>
#include <linux/of_address.h>
#include <linux/firmware.h>
#include <linux/interrupt.h>

#include "hwqueue_internal.h"

#define DESC_SIZE_MASK	0xful
#define DESC_PTR_MASK	(~DESC_SIZE_MASK)

#define THRESH_GTE	BIT(8)
#define THRESH_LT	0

#define PDSP_CTRL_PC_MASK	0xffff0000
#define PDSP_CTRL_SOFT_RESET	BIT(0)
#define PDSP_CTRL_ENABLE	BIT(1)
#define PDSP_CTRL_RUNNING	BIT(15)

struct khwq_reg_config {
	u32		revision;
	u32		__pad1;
	u32		divert;
	u32		link_ram_base0;
	u32		link_ram_size0;
	u32		link_ram_base1;
	u32		__pad2[2];
	u32		starvation[0];
};

struct khwq_reg_region {
	u32		base;
	u32		start_index;
	u32		size_count;
	u32		__pad;
};

struct khwq_reg_queue {
	u32		entry_count;
	u32		byte_count;
	u32		packet_size;
	u32		ptr_size_thresh;
};

struct khwq_reg_pdsp_regs {
	u32		control;
	u32		status;
	u32		cycle_count;
	u32		stall_count;
};

struct khwq_reg_pdsp_command {
	u32		command;
	u32		queue_mask;
	u32		list_phys;
	u32		queue_num;
	u32		timer_config;
};

struct khwq_region {
	unsigned	 desc_size;
	unsigned	 num_desc;
	dma_addr_t	 dma_start, dma_end;
	void		*virt_start, *virt_end;
	unsigned	 link_index;
};

struct khwq_pool_info {
	const char		*name;
	struct khwq_region	*region;
	int			 region_offset;
	int			 num_desc;
	int			 desc_size;
	struct hwqueue		*queue;
	struct list_head	 list;
};

struct khwq_link_ram_block {
	dma_addr_t	 phys;
	void		*virt;
	size_t		 size;
};

struct khwq_pdsp_info {
	const char				*name;
	struct khwq_reg_pdsp_regs  __iomem	*regs;
	struct khwq_reg_pdsp_command  __iomem	*command;
	u32 __iomem				*iram;
	const char				*firmware;
	struct list_head			 list;
};

struct khwq_range_info {
	const char		*name;
	unsigned		 queue_base;
	unsigned		 num_queues;
	unsigned		 irq_base;
	unsigned		 flags;
	struct list_head	 list;
};

#define RANGE_RESERVED		BIT(0)
#define RANGE_HAS_IRQ		BIT(1)

struct khwq_device {
	struct device			*dev;
	struct hwqueue_device		 hdev;
	struct khwq_region		*regions;
	resource_size_t			 start_region, num_regions;
	resource_size_t			 start_index, num_index;
	struct khwq_link_ram_block	 link_rams[2];
	unsigned			 num_queues;
	unsigned			 base_id;
	struct list_head		 queue_ranges;
	struct list_head		 pools;
	struct list_head		 pdsps;

	struct khwq_reg_config __iomem	*reg_config;
	struct khwq_reg_region __iomem	*reg_region;
	struct khwq_reg_queue __iomem	*reg_push, *reg_pop, *reg_peek;
	void __iomem			*reg_status;
};

struct khwq_instance {
	struct khwq_region	*last; /* cache last region used */
	int			 irq_num; /*irq num -ve for non-irq queues */
	char			 irq_name[32];
};

#define to_hdev(_kdev)		(&(_kdev)->hdev)
#define from_hdev(_hdev)	container_of(_hdev, struct khwq_device, hdev)

#define region_index(kdev, r)	((r) - (kdev)->regions + (kdev)->start_region)

#define for_each_region(kdev, r)				\
	for ((r) = (kdev)->regions;				\
	     (r) < (kdev)->regions + (kdev)->num_regions;	\
	     (r)++)

#define for_each_queue_range(kdev, range)			\
	list_for_each_entry(range, &kdev->queue_ranges, list)

#define for_each_pool(kdev, pool)				\
	list_for_each_entry(pool, &kdev->pools, list)

#define for_each_pdsp(kdev, pdsp)				\
	list_for_each_entry(pdsp, &kdev->pdsps, list)

static inline int khwq_pdsp_wait(u32 *addr, unsigned timeout, u32 flags)
{
	unsigned long end_time;
	u32 val;
	int ret;

	end_time = jiffies + msecs_to_jiffies(timeout);
	while (jiffies < end_time) {
		val = __raw_readl(addr);
		if (flags)
			val &= flags;
		if (!val)
			break;
		cpu_relax();
	}
	ret = val ? -ETIMEDOUT : 0;

	return ret;
}

static inline struct khwq_range_info *
khwq_find_queue_range(struct hwqueue_instance *inst)
{
	struct khwq_device *kdev = from_hdev(inst->hdev);
	unsigned id = hwqueue_inst_to_id(inst);
	struct khwq_range_info *range;

	for_each_queue_range(kdev, range)
		if (id >= range->queue_base &&
		    id < range->queue_base + range->num_queues)
			return range;
	return NULL;
}

static int khwq_match(struct hwqueue_instance *inst, unsigned flags)
{
	struct khwq_range_info *range;
	int score = 0;

	range = khwq_find_queue_range(inst);
	if (!range)
		return -ENODEV;

	if (range->flags & RANGE_RESERVED)
		score += 1000;

	if ((range->flags & RANGE_HAS_IRQ) &&
	    !(flags & (O_HIGHTHROUGHPUT | O_LOWLATENCY)))
		score += 100;
	if (!(range->flags & RANGE_HAS_IRQ) &&
	    (flags & (O_HIGHTHROUGHPUT | O_LOWLATENCY)))
		score += 100;

	return score;
}

static irqreturn_t khwq_int_handler(int irq, void *_instdata)
{
	struct hwqueue_instance *inst = _instdata;

	hwqueue_notify(inst);

	return IRQ_HANDLED;
}

static int khwq_open(struct hwqueue_instance *inst, unsigned flags)
{
	struct khwq_instance *kq = hwqueue_inst_to_priv(inst);
	struct khwq_device *kdev = from_hdev(inst->hdev);
	unsigned id = hwqueue_inst_to_id(inst);
	struct khwq_range_info *range;
	int ret, irq_num = -1;

	/* setup threshold, status bit is set when queue depth >= 1 */
	__raw_writel(THRESH_GTE | 1, &kdev->reg_peek[id].ptr_size_thresh);

	range = khwq_find_queue_range(inst);
	if (!range)
		return -ENODEV;

	kq->irq_num = -1;

	if (range->flags & RANGE_HAS_IRQ) {
		irq_num = id - range->queue_base + range->irq_base;

		scnprintf(kq->irq_name, sizeof(kq->irq_name), "hwqueue-%d", id);
		ret = request_irq(irq_num, khwq_int_handler, 0, kq->irq_name,
				  inst);
		if (ret) {
			dev_err(kdev->dev,
				"request_irq failed for queue:%d\n", id);
			return -EINVAL;
		}
		/* disable irq at this time */
		disable_irq(irq_num);
		kq->irq_num = irq_num;
	}
	return 0;
}

static void khwq_close(struct hwqueue_instance *inst)
{
	struct khwq_instance *kq = hwqueue_inst_to_priv(inst);

	if (kq->irq_num >= 0)
		free_irq(kq->irq_num, inst);
}

static void khwq_set_notify(struct hwqueue_instance *inst, bool enabled)
{
	struct khwq_instance *kq = hwqueue_inst_to_priv(inst);

	if (kq->irq_num >= 0) {
		if (enabled)
			enable_irq(kq->irq_num);
		else
			disable_irq_nosync(kq->irq_num);
	} else
		hwqueue_set_poll(inst, enabled);
}

static inline struct khwq_region *
khwq_find_region_by_virt(struct khwq_device *kdev, struct khwq_instance *inst,
			 void *virt)
{
	struct khwq_region *region;

	if (inst && inst->last &&
	    inst->last->virt_start <= virt &&
	    inst->last->virt_end > virt)
		return inst->last;

	for_each_region(kdev, region) {
		if (region->virt_start <= virt &&
		    region->virt_end > virt) {
			inst->last = region;
			return inst->last;
		}
	}

	return NULL;
}

static inline struct khwq_region *
khwq_find_region_by_dma(struct khwq_device *kdev, struct khwq_instance *inst,
			dma_addr_t dma)
{
	struct khwq_region *region;

	if (inst && inst->last &&
	    inst->last->dma_start <= dma &&
	    inst->last->dma_end > dma)
		return inst->last;

	for_each_region(kdev, region) {
		if (region->dma_start <= dma &&
		    region->dma_end > dma) {
			inst->last = region;
			return inst->last;
		}
	}

	return NULL;
}

static int khwq_push(struct hwqueue_instance *inst, void *data, unsigned size)
{
	struct khwq_device *kdev = from_hdev(inst->hdev);
	struct khwq_instance *kq = hwqueue_inst_to_priv(inst);
	unsigned id = hwqueue_inst_to_id(inst);
	struct khwq_region *region;
	dma_addr_t dma;
	u32 desc_size;
	u32 val;

	region = khwq_find_region_by_virt(kdev, kq, data);
	if (!region)
		return -EINVAL;

	desc_size = min(size, region->desc_size);
	dma = region->dma_start + (data - region->virt_start);

	if (WARN_ON(dma & DESC_SIZE_MASK))
		return -EINVAL;

	dma_sync_single_for_device(kdev->dev, dma, desc_size, DMA_TO_DEVICE);

	val = (u32)dma | ((desc_size / 16) - 1);

	__raw_writel(val, &kdev->reg_push[id].ptr_size_thresh);
	return 0;
}

static void *khwq_pop(struct hwqueue_instance *inst, unsigned *size)
{
	struct khwq_device *kdev = from_hdev(inst->hdev);
	struct khwq_instance *kq = hwqueue_inst_to_priv(inst);
	unsigned id = hwqueue_inst_to_id(inst);
	struct khwq_region *region;
	u32 val, desc_size;
	dma_addr_t dma;
	void *data;

	val = __raw_readl(&kdev->reg_pop[id].ptr_size_thresh);
	if (!val)
		return NULL;
	dma = val & DESC_PTR_MASK;
	desc_size = ((val & DESC_SIZE_MASK) + 1) * 16;

	region = khwq_find_region_by_dma(kdev, kq, dma);
	if (!region)
		return ERR_PTR(-EINVAL);

	desc_size = min(desc_size, region->desc_size);

	data = region->virt_start + (dma - region->dma_start);
	dma_sync_single_for_cpu(kdev->dev, dma, desc_size, DMA_FROM_DEVICE);

	if (size)
		*size = desc_size;

	return data;
}

static bool khwq_is_empty(struct hwqueue_instance *inst)
{
	struct khwq_device *kdev = from_hdev(inst->hdev);
	unsigned id = hwqueue_inst_to_id(inst);
	u32 val;

	val = __raw_readl(kdev->reg_status + (id % BITS_PER_LONG));

	return (val & BIT_MASK(id)) ? true : false;
}

static int khwq_flush(struct hwqueue_instance *inst)
{
	struct khwq_device *kdev = from_hdev(inst->hdev);
	unsigned id = hwqueue_inst_to_id(inst);

	__raw_writel(0, &kdev->reg_push[id].ptr_size_thresh);
	return 0;
}

static struct hwqueue_device_ops khdev_ops = {
	.match		= khwq_match,
	.open		= khwq_open,
	.set_notify	= khwq_set_notify,
	.close		= khwq_close,
	.push		= khwq_push,
	.pop		= khwq_pop,
	.is_empty	= khwq_is_empty,
	.flush		= khwq_flush,
};

static __devinit struct khwq_region *
khwq_find_match_region(struct khwq_device *kdev, struct khwq_pool_info *pool)
{
	struct khwq_region *region;

	for_each_region(kdev, region) {
		if (!region->desc_size)
			continue;
		if (region->desc_size != pool->desc_size)
			continue;

		/* TODO: other checks here - e.g. mem types, fixed pools */

		return region;
	}
	return NULL;
}

static __devinit struct khwq_region *
khwq_find_free_region(struct khwq_device *kdev, struct khwq_pool_info *pool)
{
	struct khwq_region *region;

	for_each_region(kdev, region)
		if (!region->desc_size)
			return region;
	return NULL;
}

/* Map the requested pools to regions, creating regions as we go */
static void __devinit khwq_map_pools(struct khwq_device *kdev)
{
	struct khwq_region *region;
	struct khwq_pool_info *pool;
	unsigned size;

	for_each_pool(kdev, pool) {
		/* figure out alignment needs for descriptors */
		size = max(16, dma_get_cache_alignment());
		pool->desc_size	= ALIGN(pool->desc_size, size);

		/* recalculate pool size post alignment */
		size = pool->desc_size * pool->num_desc;

		/* realign pool to page size */
		size = ALIGN(size, PAGE_SIZE);

		/* finally recalculate number of descriptors... phew */
		pool->num_desc = size / pool->desc_size;

		/* find a matching region, otherwise find a free region */
		region = khwq_find_match_region(kdev, pool);
		if (!region)
			region = khwq_find_free_region(kdev, pool);

		if (!region) {
			dev_err(kdev->dev, "failed to set up pool %s\n",
				pool->name);
			continue;
		}

		/* link this pool to the region... */
		pool->region = region;
		pool->region_offset = region->num_desc;

		/* ... and inflate the region accordingly */
		region->desc_size = pool->desc_size;
		region->num_desc += pool->num_desc;

		dev_dbg(kdev->dev, "pool %s: num:%d, size:%d, region:%d\n",
			pool->name, pool->num_desc, pool->desc_size,
			region_index(kdev, region));
	}
}

static int __devinit khwq_setup_region(struct khwq_device *kdev,
				       struct khwq_region *region,
				       unsigned start_index,
				       unsigned max_descs)
{
	unsigned hw_num_desc, hw_desc_size, size;
	int id = region_index(kdev, region);
	struct khwq_reg_region __iomem  *regs = kdev->reg_region + id;

	/* unused region? */
	if (!region->num_desc)
		return 0;

	max_descs = rounddown_pow_of_two(max_descs);

	/* round up num_desc to hardware needs, i.e. 2^(n+5) */
	region->num_desc = max(32u, region->num_desc);
	hw_num_desc = ilog2(region->num_desc - 1) + 1;
	region->num_desc = 1 << hw_num_desc;

	/* force fit this region into available link ram index range */
	region->num_desc = min(max_descs, region->num_desc);

	/* did we force fit ourselves into nothingness? */
	if (region->num_desc < 32) {
		region->num_desc = 0;
		return 0;
	}

	/* reserve link ram index range for this region */
	region->link_index = start_index;

	size = region->num_desc * region->desc_size;
	region->virt_start = dmam_alloc_coherent(kdev->dev, size,
				&region->dma_start, GFP_KERNEL);
	if (!region->virt_start) {
		region->num_desc = 0;
		return 0;
	}

	region->virt_end = region->virt_start + size;
	region->dma_end  = region->dma_start + size;

	dev_dbg(kdev->dev,
		"region %d: num:%d, size:%d, link:%d, phys:%08x, virt:%p\n",
		id, region->num_desc, region->desc_size, region->link_index,
		region->dma_start, region->virt_start);

	hw_desc_size = (region->desc_size / 16) - 1;
	hw_num_desc -= 5;

	__raw_writel(region->dma_start, &regs->base);
	__raw_writel(start_index, &regs->start_index);
	__raw_writel(hw_desc_size << 16 | hw_num_desc, &regs->size_count);

	return region->num_desc;
}

static int __devinit khwq_setup_regions(struct khwq_device *kdev)
{
	unsigned size, link_index = 0;
	struct khwq_region *region;

	size = kdev->num_regions * sizeof(struct khwq_region);
	kdev->regions = devm_kzalloc(kdev->dev, size, GFP_KERNEL);
	if (!kdev->regions)
		return -ENOMEM;

	khwq_map_pools(kdev);

	 /* Next, we run through the regions and set things up */
	for_each_region(kdev, region) {
		link_index += khwq_setup_region(kdev, region,
					kdev->start_index + link_index,
					kdev->num_index - link_index);
	}

	return 0;
}

/* carve out descriptors and push into named queues */
static void __devinit khwq_setup_pools(struct khwq_device *kdev)
{
	struct khwq_region *region;
	struct khwq_pool_info *pool;
	int ret, i;

	for_each_pool(kdev, pool) {
		pool->queue = hwqueue_open(pool->name, HWQUEUE_ANY,
					   O_CREAT | O_RDWR | O_NONBLOCK);
		if (IS_ERR_OR_NULL(pool->queue)) {
			dev_err(kdev->dev,
				"failed to open queue for pool %s, error %ld\n",
				pool->name, PTR_ERR(pool->queue));
			pool->queue = NULL;
			continue;
		}

		region = pool->region;

		if (!region || !region->num_desc) {
			dev_err(kdev->dev, "no region for pool %s\n",
				pool->name);
			continue;
		}

		pool->desc_size = region->desc_size;
		for (i = 0; i < pool->num_desc; i++) {
			int index = pool->region_offset + i;
			void *desc;

			desc = region->virt_start + region->desc_size * index;
			ret = hwqueue_push(pool->queue, desc, pool->desc_size);
			WARN_ONCE(ret, "failed push to pool queue %s\n",
				  pool->name);
		}
	}
}

static int __devinit khwq_get_link_ram(struct khwq_device *kdev,
				       const char *name,
				       struct khwq_link_ram_block *block)
{
	struct platform_device *pdev = to_platform_device(kdev->dev);
	struct device_node *node = pdev->dev.of_node;
	u32 temp[2];

	/*
	 * Note: link ram resources are specified in "entry" sized units. In
	 * reality, although entries are ~40bits in hardware, we treat them as
	 * 64-bit entities here.
	 *
	 * For example, to specify the internal link ram for Keystone-I class
	 * devices, we would set the linkram0 resource to 0x80000-0x83fff.
	 *
	 * This gets a bit weird when other link rams are used.  For example,
	 * if the range specified is 0x0c000000-0x0c003fff (i.e., 16K entries
	 * in MSMC SRAM), the actual memory used is 0x0c000000-0x0c020000,
	 * which accounts for 64-bits per entry, for 16K entries.
	 */
	if (!of_property_read_u32_array(node, name , temp, 2)) {
		if (temp[0]) {
			/*
			 * queue_base specified => using internal or onchip
			 * link ram WARNING - we do not "reserve" this block
			 */
			block->phys = (dma_addr_t)temp[0];
			block->virt = NULL;
			block->size = temp[1];
		} else {
			block->size = temp[1];
			/* queue_base not specific => allocate requested size */
			block->virt = dmam_alloc_coherent(kdev->dev,
					8 * block->size, &block->phys,
					GFP_KERNEL);
			if (!block->virt) {
				dev_err(kdev->dev, "failed to alloc linkram\n");
				return -ENOMEM;
			}
		}
	} else
		return -ENODEV;
	return 0;
}

static int __devinit khwq_setup_link_ram(struct khwq_device *kdev)
{
	struct khwq_link_ram_block *block = &kdev->link_rams[0];

	dev_dbg(kdev->dev, "linkram0: phys:%x, virt:%p, size:%x\n",
		block->phys, block->virt, block->size);
	__raw_writel(block->phys, &kdev->reg_config->link_ram_base0);
	__raw_writel(block->size, &kdev->reg_config->link_ram_size0);

	block++;
	if (!block->size)
		return 0;

	dev_dbg(kdev->dev, "linkram1: phys:%x, virt:%p, size:%x\n",
		block->phys, block->virt, block->size);
	__raw_writel(block->phys, &kdev->reg_config->link_ram_base1);

	return 0;
}

static const char *khwq_find_name(struct device_node *node)
{
	const char *name;

	if (of_property_read_string(node, "label", &name) < 0)
		name = node->name;
	if (!name)
		name = "unknown";
	return name;
}

static int khwq_init_queue_ranges(struct khwq_device *kdev,
				  struct device_node *queues)
{
	struct device *dev = kdev->dev;
	struct khwq_range_info *range;
	struct device_node *child;
	u32 temp[2];
	int ret;

	for_each_child_of_node(queues, child) {

		range = devm_kzalloc(dev, sizeof(*range), GFP_KERNEL);
		if (!range) {
			dev_err(dev, "out of memory allocating range\n");
			return -ENOMEM;
		}

		range->name = khwq_find_name(child);

		ret = of_property_read_u32_array(child, "values", temp, 2);
		if (!ret) {
			range->queue_base = temp[0] - kdev->base_id;
			range->num_queues = temp[1];
		} else {
			dev_err(dev, "invalid queue range %s\n", range->name);
			devm_kfree(dev, range);
			continue;
		}

		ret = of_property_read_u32(child, "irq-base", &range->irq_base);
		if (ret >= 0)
			range->flags |= RANGE_HAS_IRQ;

		if (of_get_property(child, "reserved", NULL))
			range->flags |= RANGE_RESERVED;

		list_add_tail(&range->list, &kdev->queue_ranges);

		dev_dbg(dev, "added range %s: %d-%d, irqs %d-%d%s%s\n",
			range->name, range->queue_base,
			range->queue_base + range->num_queues - 1,
			range->irq_base,
			range->irq_base + range->num_queues - 1,
			(range->flags & RANGE_HAS_IRQ) ? ", has irq" : "",
			(range->flags & RANGE_RESERVED) ? ", reserved" : "");
	}

	if (list_empty(&kdev->queue_ranges)) {
		dev_err(dev, "no valid queue range found\n");
		return -ENODEV;
	}

	return 0;
}

static int khwq_init_pools(struct khwq_device *kdev, struct device_node *pools)
{
	struct device *dev = kdev->dev;
	struct khwq_pool_info *pool;
	struct device_node *child;
	u32 temp[2];
	int ret;

	for_each_child_of_node(pools, child) {

		pool = devm_kzalloc(dev, sizeof(*pool), GFP_KERNEL);
		if (!pool) {
			dev_err(dev, "out of memory allocating pool\n");
			return -ENOMEM;
		}

		pool->name = khwq_find_name(child);

		ret = of_property_read_u32_array(child, "values", temp, 2);
		if (!ret) {
			pool->num_desc  = temp[0];
			pool->desc_size = temp[1];
		} else {
			dev_err(dev, "invalid queue pool %s\n", pool->name);
			devm_kfree(dev, pool);
			continue;
		}

		list_add_tail(&pool->list, &kdev->pools);

		dev_dbg(dev, "added pool %s: %d descriptors of size %d\n",
			pool->name, pool->num_desc, pool->desc_size);
	}

	if (list_empty(&kdev->pools)) {
		dev_err(dev, "no valid descriptor pool found\n");
		return -ENODEV;
	}

	return 0;
}

static int khwq_init_pdsps(struct khwq_device *kdev, struct device_node *pdsps)
{
	struct device *dev = kdev->dev;
	struct khwq_pdsp_info *pdsp;
	struct device_node *child;
	int ret;

	for_each_child_of_node(pdsps, child) {

		pdsp = devm_kzalloc(dev, sizeof(*pdsp), GFP_KERNEL);
		if (!pdsp) {
			dev_err(dev, "out of memory allocating pdsp\n");
			return -ENOMEM;
		}

		pdsp->name = khwq_find_name(child);

		ret = of_property_read_string(child, "firmware",
					      &pdsp->firmware);
		if (ret < 0 || !pdsp->firmware) {
			dev_err(dev, "unknown firmware for pdsp %s\n",
				pdsp->name);
			kfree(pdsp);
			continue;
		}
		dev_dbg(dev, "pdsp name %s fw name :%s\n",
		       pdsp->name, pdsp->firmware);

		pdsp->iram	= of_iomap(child, 0);
		pdsp->regs	= of_iomap(child, 1);
		pdsp->command	= of_iomap(child, 2);
		if (!pdsp->command || !pdsp->iram || !pdsp->regs) {
			dev_err(dev, "failed to map pdsp %s regs\n",
				pdsp->name);
			if (pdsp->command)
				devm_iounmap(dev, pdsp->command);
			if (pdsp->iram)
				devm_iounmap(dev, pdsp->iram);
			if (pdsp->regs)
				devm_iounmap(dev, pdsp->regs);
			kfree(pdsp);
			continue;
		}

		list_add_tail(&pdsp->list, &kdev->pdsps);

		dev_dbg(dev, "added pdsp %s: command %p, iram %p, "
			 "regs %p, firmware %s\n",
			 pdsp->name, pdsp->command, pdsp->iram, pdsp->regs,
			 pdsp->firmware);
	}

	return 0;
}


static int khwq_stop_pdsp(struct khwq_device *kdev,
			  struct khwq_pdsp_info *pdsp)
{
	u32 val, timeout = 1000;
	int ret;

	val = __raw_readl(&pdsp->regs->control) & ~PDSP_CTRL_ENABLE;
	__raw_writel(val, &pdsp->regs->control);

	ret = khwq_pdsp_wait(&pdsp->regs->control, timeout, PDSP_CTRL_RUNNING);

	if (ret < 0) {
		dev_err(kdev->dev, "timed out on pdsp %s stop\n", pdsp->name);
		return ret;
	}
	return 0;
}

static int khwq_load_pdsp(struct khwq_device *kdev,
			  struct khwq_pdsp_info *pdsp)
{
	int i, ret, fwlen;
	const struct firmware *fw;
	u32 *fwdata;

	ret = request_firmware(&fw, pdsp->firmware, kdev->dev);
	if (ret) {
		dev_err(kdev->dev, "failed to get firmware %s for pdsp %s\n",
			pdsp->firmware, pdsp->name);
		return ret;
	}

	/* download the firmware */
	fwdata = (u32 *)fw->data;
	fwlen = (fw->size + sizeof(u32) - 1) / sizeof(u32);

	for (i = 0; i < fwlen; i++)
		__raw_writel(fwdata[i], pdsp->iram + i);

	release_firmware(fw);

	return 0;
}

static int khwq_start_pdsp(struct khwq_device *kdev,
			   struct khwq_pdsp_info *pdsp)
{
	u32 val, timeout = 1000;
	int ret;

	/* write a command for sync */
	__raw_writel(0xffffffff, &pdsp->command->command);
	while (__raw_readl(&pdsp->command->command) != 0xffffffff)
		cpu_relax();

	/* soft reset the PDSP */
	val  = __raw_readl(&pdsp->regs->control);
	val &= ~(PDSP_CTRL_PC_MASK | PDSP_CTRL_SOFT_RESET);
	__raw_writel(val, &pdsp->regs->control);

	/* enable pdsp */
	val = __raw_readl(&pdsp->regs->control) | PDSP_CTRL_ENABLE;
	__raw_writel(val, &pdsp->regs->control);

	/* wait for command register to clear */
	ret = khwq_pdsp_wait(&pdsp->command->command, timeout, 0);

	if (ret < 0) {
		dev_err(kdev->dev, "timed out on pdsp %s command register wait\n",
			pdsp->name);
		return ret;
	}
	return 0;
}

static int khwq_start_pdsps(struct khwq_device *kdev)
{
	struct khwq_pdsp_info *pdsp;
	int ret;

	/* disable all pdsps */
	for_each_pdsp(kdev, pdsp)
		khwq_stop_pdsp(kdev, pdsp);

	/* now load them all */
	for_each_pdsp(kdev, pdsp) {
		ret = khwq_load_pdsp(kdev, pdsp);
		if (ret < 0)
			return ret;
	}

	for_each_pdsp(kdev, pdsp) {
		ret = khwq_start_pdsp(kdev, pdsp);
		WARN_ON(ret);
	}

	return 0;
}

static int __devinit khwq_probe(struct platform_device *pdev)
{
	struct device_node *node = pdev->dev.of_node;
	struct device_node *pdsps, *descs, *queues;
	struct device *dev = &pdev->dev;
	struct hwqueue_device *hdev;
	struct khwq_device *kdev;
	u32 temp[2];
	int ret;

	if (!node) {
		dev_err(dev, "device tree info unavailable\n");
		return -ENODEV;
	}

	queues = of_find_child_by_name(node, "queues");
	if (!queues) {
		dev_err(dev, "queues not specified\n");
		return -ENODEV;
	}
	pdsps =  of_find_child_by_name(node, "pdsps");
	BUG_ON(!pdsps);
	if (!pdsps) {
		dev_err(dev, "pdsp info not specified\n");
	}
	descs =  of_find_child_by_name(node, "descriptors");
	if (!descs) {
		dev_err(dev, "descriptor pools not specified\n");
		return -ENODEV;
	}

	kdev = devm_kzalloc(dev, sizeof(struct khwq_device), GFP_KERNEL);
	if (!kdev) {
		dev_err(dev, "memory allocation failed\n");
		return -ENOMEM;
	}

	platform_set_drvdata(pdev, kdev);
	kdev->dev = dev;
	INIT_LIST_HEAD(&kdev->queue_ranges);
	INIT_LIST_HEAD(&kdev->pools);
	INIT_LIST_HEAD(&kdev->pdsps);

	if (of_property_read_u32_array(node, "range", temp, 2)) {
		dev_err(dev, "hardware queue range not specified\n");
		return -ENODEV;
	}
	kdev->base_id    = temp[0];
	kdev->num_queues = temp[1];

	/* get usable queue range values from device tree */
	ret = khwq_init_queue_ranges(kdev, queues);
	if (ret)
		return ret;

	/* get pdsp configuration values from device tree */
	if (pdsps) {
		ret = khwq_init_pdsps(kdev, pdsps);
		if (ret)
			return ret;

		ret = khwq_start_pdsps(kdev);
		if (ret)
			return ret;
	}

	/* Get descriptor pool values from device tree */
	ret = khwq_init_pools(kdev, descs);
	if (ret)
		return ret;

	kdev->reg_peek		= of_devm_iomap(dev, 0);
	kdev->reg_status	= of_devm_iomap(dev, 1);
	kdev->reg_config	= of_devm_iomap(dev, 2);
	kdev->reg_region	= of_devm_iomap(dev, 3);
	kdev->reg_push		= of_devm_iomap(dev, 4);
	kdev->reg_pop		= of_devm_iomap(dev, 5);

	if (!kdev->reg_pop) {
		dev_dbg(kdev->dev, "defaulting pop regs to push\n");
		kdev->reg_pop = kdev->reg_push;
	}

	if (!kdev->reg_peek || !kdev->reg_status || !kdev->reg_config ||
	    !kdev->reg_region || !kdev->reg_push || !kdev->reg_pop) {
		dev_err(dev, "failed to set up register areas\n");
		return -ENOMEM;
	}

	if (!of_property_read_u32_array(node, "regions", temp, 2)) {
		kdev->start_region = temp[0];
		kdev->num_regions = temp[1];
	}

	BUG_ON(!kdev->num_regions);
	dev_dbg(kdev->dev, "regions: %d-%d\n", kdev->start_region,
		 kdev->start_region + kdev->num_regions - 1);

	if (!of_property_read_u32_array(node, "link-index", temp, 2)) {
		kdev->start_index = temp[0];
		kdev->num_index = temp[1];
	}

	BUG_ON(!kdev->num_index);
	dev_dbg(kdev->dev, "link-index: %d-%d\n", kdev->start_index,
		kdev->start_index + kdev->num_index - 1);

	ret = khwq_get_link_ram(kdev, "linkram0", &kdev->link_rams[0]);
	if (ret) {
		dev_err(kdev->dev, "could not setup linking ram\n");
		return ret;
	}

	ret = khwq_get_link_ram(kdev, "linkram1", &kdev->link_rams[1]);
	if (ret) {
		/*
		 * nothing really, we have one linking ram already, so we just
		 * live within our means
		 */
	}

	ret = khwq_setup_link_ram(kdev);
	if (ret)
		return ret;

	ret = khwq_setup_regions(kdev);
	if (ret)
		return ret;

	/* initialize hwqueue device data */
	hdev = to_hdev(kdev);
	hdev->dev	 = dev;
	hdev->base_id	 = kdev->base_id;
	hdev->num_queues = kdev->num_queues;
	hdev->priv_size	 = sizeof(struct khwq_instance);
	hdev->ops	 = &khdev_ops;

	/* register the hwqueue device */
	ret = hwqueue_device_register(hdev);
	if (ret < 0) {
		dev_err(dev, "hwqueue registration failed\n");
		return ret;
	}

	khwq_setup_pools(kdev);

	return 0;
}

static int __devexit khwq_remove(struct platform_device *pdev)
{
	struct khwq_device *kdev = platform_get_drvdata(pdev);
	struct hwqueue_device *hdev = to_hdev(kdev);
	int ret;

	ret = hwqueue_device_unregister(hdev);
	if (ret < 0) {
		dev_err(&pdev->dev, "hwqueue unregistration failed\n");
		return ret;
	}

	return 0;
}

/* Match table for of_platform binding */
static struct of_device_id __devinitdata keystone_hwqueue_of_match[] = {
	{ .compatible = "ti,keystone-hwqueue", },
	{},
};
MODULE_DEVICE_TABLE(of, keystone_hwqueue_of_match);

static struct platform_driver keystone_hwqueue_driver = {
	.probe		= khwq_probe,
	.remove		= __devexit_p(khwq_remove),
	.driver		= {
		.name	= "keystone-hwqueue",
		.owner	= THIS_MODULE,
		.of_match_table = keystone_hwqueue_of_match,
	},
};

static int __init keystone_hwqueue_init(void)
{
	return platform_driver_register(&keystone_hwqueue_driver);
}
subsys_initcall(keystone_hwqueue_init);

static void __exit keystone_hwqueue_exit(void)
{
	platform_driver_unregister(&keystone_hwqueue_driver);
}
module_exit(keystone_hwqueue_exit);

MODULE_LICENSE("GPL v2");
MODULE_DESCRIPTION("Hardware queue driver for Keystone devices");
